{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxAMCWogSeM2",
        "outputId": "fefe881f-859a-4b24-bc86-6deae939bcbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T8HKQUBPR_xy",
        "outputId": "813e8aac-21fa-41da-b43c-fc381815a88c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Text Sample:\n",
            " DhariniPV\n",
            "Madurai,TamilNadu625003\n",
            "+919843380803\n",
            "dharinipv@gmail.com\n",
            "linkedin.com/in/dharini-pv/\n",
            "github.com/DHARINIPV\n",
            "Summary\n",
            "PassionateAIandDataenthusiastskilledinPython,Object-OrientedProgramming(OOP),andDataStructures,withhands-onexperiencein\n",
            "buildingreal-timeapplicationsandautomationtools.Proficientinfilehandling,debugging,anddataprocessingusingPandasandScikit-\n",
            "learn.AdeptatworkingwithAPIs,webservices,anddatabases.Committedtowritingclean,documentedcodeandusingGitfor\n",
            "collaborativedevelopment.\n",
            "\n",
            "\n",
            "Extracted Structured Information:\n",
            "\n",
            "{\n",
            "  \"Name\": \"linkedin.com/in/dharini-pv/\",\n",
            "  \"Email\": \"dharinipv@gmail.com\",\n",
            "  \"Skills\": [\n",
            "    \"Python\",\n",
            "    \"Java\",\n",
            "    \"C\",\n",
            "    \"SQL\",\n",
            "    \"Pandas\",\n",
            "    \"Matplotlib\",\n",
            "    \"Seaborn\",\n",
            "    \"Scikit-learn\",\n",
            "    \"Excel\",\n",
            "    \"Scikit-learn\"\n",
            "  ],\n",
            "  \"Degree\": \"B.Tech\",\n",
            "  \"Institution\": \"College 2022\\u20132026\",\n",
            "  \"Work_Experience\": \"PassionateAIandDataenthusiastskilledinPython,Object-OrientedProgramming(OOP),andDataStructures,withhands-onexperiencein\\nInternship\\nMachine Learning Intern June 2024 - July 2024\\nMachineLearningInternshipinRETECHSolutionPVTLTD June \\u2013 July 2024\\nNPTEL Certification \\u2013 Introduction to Internet Of Things January - April 2024\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "pdf_path = \"/content/dharinipv_resume.pdf\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_file) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "resume_text = extract_text_from_pdf(pdf_path)\n",
        "print(\"Extracted Text Sample:\\n\", resume_text[:500])\n",
        "doc = nlp(resume_text)\n",
        "\n",
        "name_candidate = None\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == \"PERSON\":\n",
        "        name_candidate = ent.text\n",
        "        break\n",
        "\n",
        "email = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", resume_text)\n",
        "email = email.group(0) if email else None\n",
        "\n",
        "skills_list = [\n",
        "    \"Python\", \"Java\", \"C++\",\"C\",\"JavaScript\", \"SQL\", \"React.js\", \"Node.js\",\n",
        "    \"Express.js\", \"MongoDB\", \"TensorFlow\", \"EfficientNet\", \"ResNet\",\n",
        "    \"Random Forest\", \"XGBoost\", \"Docker\", \"Firebase\", \"Figma\",\"Pandas\", \"NumPy\",\n",
        "    \"Matplotlib\", \"Seaborn\", \"Scikit-learn\", \"SciPy\", \"StatsModels\", \"Excel\", \"Jupyter\", \"Power BI\", \"Tableau\", \"Looker\",\n",
        "    \"TensorFlow\", \"Keras\", \"PyTorch\", \"Scikit-learn\", \"OpenCV\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"spaCy\", \"NLTK\", \"Transformers\",\n",
        "    \"YOLO\", \"BERT\", \"GPT\", \"FastText\", \"Gensim\"\n",
        "]\n",
        "\n",
        "found_skills = [skill for skill in skills_list if skill.lower() in resume_text.lower()]\n",
        "\n",
        "degree = None\n",
        "institution = None\n",
        "\n",
        "degree_patterns = [\"B\\.Tech\", \"M\\.Tech\", \"Bachelors?\", \"Masters?\", \"Bachelor of\", \"Master of\"]\n",
        "for pattern in degree_patterns:\n",
        "    match = re.search(pattern, resume_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        degree = match.group()\n",
        "        break\n",
        "\n",
        "institution_pattern = r\"(College|University|Institute)[^\\n]+\"\n",
        "inst_match = re.search(institution_pattern, resume_text)\n",
        "if inst_match:\n",
        "    institution = inst_match.group()\n",
        "\n",
        "experience_section = \"\"\n",
        "for line in resume_text.splitlines():\n",
        "    if \"intern\" in line.lower() or \"experience\" in line.lower():\n",
        "        experience_section += line + \"\\n\"\n",
        "\n",
        "structured_resume = {\n",
        "    \"Name\": name_candidate,\n",
        "    \"Email\": email,\n",
        "    \"Skills\": found_skills,\n",
        "    \"Degree\": degree,\n",
        "    \"Institution\": institution,\n",
        "    \"Work_Experience\": experience_section.strip()\n",
        "}\n",
        "\n",
        "import json\n",
        "print(\"\\nExtracted Structured Information:\\n\")\n",
        "print(json.dumps(structured_resume, indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}